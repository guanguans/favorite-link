# Trae AI Coding 模型评估指南

> 本文档用于指导 AI 助手为项目生成符合 Trae 评估标准的任务 prompt

## 📋 项目概览

**项目性质：** Trae AI Coding 工具的模型评估与对比测试
**评估目标：** 让 Trae 执行编程任务 → 观察表现 → 根据规范打分并写理由
**核心价值：** 通过真实开发场景测试模型的代码生成能力、工具调用效率和用户体验

---

## 🎯 AI 助手使用指令

当用户要求为某个项目生成 Trae 评估任务时，你必须：

1. **深度分析项目**
   - 阅读项目的 README.md、package.json/composer.json 等配置文件
   - 理解项目的技术栈、核心功能、架构模式
   - 识别项目的潜在改进点和扩展方向

2. **生成符合标准的任务 prompt**
   - 严格遵守本文档的"任务构造标准"章节
   - 提供 2-3 个不同难度的方案供用户选择
   - 每个方案必须包含：任务描述、适合理由、预估代码量、预估工时

3. **输出格式要求**
   - 使用自然语言描述，贴近日常开发沟通
   - 避免 AI 味过浓的 Markdown/JSON 格式
   - 包含明确的验证步骤和预期结果

4. **质量检查清单**
   - ✅ 符合真实开发场景
   - ✅ 复杂度适中（不过简，不刁难）
   - ✅ 可在 2-3 轮对话内完成
   - ✅ 有明确的验收标准
   - ❌ 不是从 0 开始构建项目
   - ❌ 不是重复性套题（游戏合集类）
   - ❌ 不是复杂系统堆砌

---

## 📐 任务构造标准

### 硬性要求

| 要求项 | 标准 |
|--------|------|
| **任务分类** | 必须属于以下三种之一：<br>• 界面功能：创建新的 UI 组件、页面或交互功能<br>• 运行时异常修复：修复空指针、网络超时等运行时异常<br>• 业务逻辑：实现新的业务处理流程、规则或算法 |
| **对话轮次** | 试标：最多 2 轮<br>正式项目：最多 3 轮（具体以需求为准） |
| **复杂度要求** | 满足以下任一类型：<br>**类型 1**：50-300 行代码，单一框架/语言，1-4 小时<br>**类型 2**：≥300 行代码（多文件），跨技术栈（≥3 种），≥4 小时 |
| **场景真实性** | 必须符合真实开发场景，不得刻意刁难或构造不合理需求 |

### 建议要求

- 任务需要具备一定复杂度，能够触发 Model Breaking（模型需要人工介入的情况）
- 任务描述清晰完整，包含具体的功能点和验收标准
- 避免过于简单的任务（如"设置暗色模式"、"翻译项目"）
- 避免过于宽泛的任务（如"优化性能"、"重构代码"）

---

## ⭐ 评分维度与标准

### GSB 整体对比标准

**什么是 GSB？**
在相同任务下，评测目标产品与竞品的表现差异：
- **Good（更好）**：A 模型更好 / B 模型更好
- **Same（一样）**：一样好 / 一样差

**评判原则：**
- 优先考虑使用过程清晰、高效、不拖沓的模型
- 其次考虑交付物质量（符合要求、能运行、代码质量）
- 当两个模型各有优劣时，优先选择过程更好且交付物可运行的一方
- 当两个模型都有严重缺陷时，评为"一样差"

---

### 三大核心维度（1-5 星评分）

#### 1. 交付完整性

| 分数 | 标准 |
|------|------|
| ⭐⭐⭐⭐⭐ | 交付物不仅实现了全部需求，还根据需求完善了**隐性需求**（超预期） |
| ⭐⭐⭐⭐ | 交付物实现了全部需求 |
| ⭐⭐⭐ | 交付物达成了主要需求<br>非代码交付题的解答解决了主要问题，但内容略显简略 |
| ⭐⭐ | 交付物具备可运行性，但未达成主要需求<br>因环境依赖问题无法运行，但代码本身已实现主要需求<br>非代码交付题的回答模糊含混、缺乏针对性 |
| ⭐ | 代码编译失败，且频繁出现错误，无法运行<br>非代码交付题的回答偏离主题，未能解决主要问题 |

**重要：** 如果完整性评分低于 4 分，必须详细分析原因，避免空话套话。

**示例（好的分析）：**
```
表头吸顶：部分有效，但实现存在关键问题
• 本次提交给 th 加的是 left: 0（会造成"冻结首列"效果），而不是 top: 0，
  方向错误，可能导致吸顶无效或样式错乱

滚动加载：方向正确，但不满足"无限滚动"的预期
• 当前实现只是"滚动触发翻页"（更新 pagination.pageIndex），没有"累积追加数据"
• 用户不会看到"行数不断增加"的效果，而是单页内容被替换
```

**示例（差的分析）：**
```
❌ "模型思考次数到达上限，可能是因为功能涉及多模块设计..."
❌ "两者表现的都还凑合"
```

---

#### 2. 工具调用

| 分数 | 标准 |
|------|------|
| ⭐⭐⭐⭐⭐ | 自主调用合适的工具并完美使用 |
| ⭐⭐⭐⭐ | 能正确使用工具，但存在轻微的效率问题或 ~20% 的调用失败 |
| ⭐⭐⭐ | 工具调用存在 20%～50% 的失败 |
| ⭐⭐ | 工具选择不当或 50% 以上工具调用被错误使用 |
| ⭐ | 即使必要时也无法使用工具 |

**如何获取工具调用日志：**
1. 点击 Trae 回复底部的"复制全部"按钮
2. 搜索 `toolName:` 出现的次数 = 工具调用次数
3. 搜索 `status: success` 出现的次数 = 调用成功次数
4. 使用工具分析：https://trae.myuan.fun/

---

#### 3. 用户体验

| 分数 | 标准 |
|------|------|
| ⭐⭐⭐⭐⭐ | 模型推理高效流畅、结构清晰、几乎无停顿或自纠环节，且能迅速抓住问题核心。输出结果让人愉快且可信，几乎无改进空间 |
| ⭐⭐⭐⭐ | 模型整体体验良好，偶有轻微冗余操作或自我修正，但不影响任务完成 |
| ⭐⭐⭐ | 模型存在一定的重复或冗余解释。存在较多的自纠环节，偶尔将简单问题复杂化；用户仍有意愿等待模型结果 |
| ⭐⭐ | 模型推理过程拖沓、反复纠正或出现明显逻辑错误。简单问题被过度复杂化；导致用户失去耐心，需要中断模型推理进程进行干预 |
| ⭐ | 模型推理步骤极其繁琐或陷入死循环且无自我修复能力。严重影响用户体验且没有任何有效阶段性产出 |

**重要：** 不应当存在用户体验得分较高，但交付完整性较低的情况。

---

## ✅ 优秀案例参考

### 案例：Flask 博客浏览历史功能

**Prompt：**
```
我这边有一个基于 Flask 的博客项目，已经实现了用户注册登录、发帖、分类、搜索、
评论、后台和访问统计等常规功能，整个站点现在可以正常打开使用，但目前没有一个
"浏览历史"的入口，我想给登录用户增加一个"最近浏览"功能，大致效果是：

1. 用户登录状态下每次打开某篇文章详情页时，后台自动记录一条浏览记录（包含用户、
   文章以及浏览时间）
2. 同一篇文章重复浏览可以更新记录时间或只保留一条，整体只保留最近一个月内的记录，
   然后在站点里新增一个只对已登录用户可见的"浏览历史"页面或入口（比如放在用户菜单里）
3. 进入后按时间倒序展示该用户最近浏览过的文章列表，每条至少要有文章标题、所属分类、
   浏览时间，点击可以跳转回对应文章

实现时请你先阅读项目现有的模型、视图函数和模板结构，选择一个和当前风格一致的方式
来增加所需的数据结构、路由和模板，不要破坏或删除与本需求无关的功能和样式，尽量控制
修改在一个合理的范围内（比如新增/修改几十到两三百行代码），完成后告诉我本地应该用
什么命令启动项目、在哪个 URL 或导航入口可以看到"浏览历史"页面，以及我可以按什么
步骤来验证这个功能是正常工作的。
```

**为什么这是好案例：**
- ✅ 功能需求明确且完整（3 个具体功能点）
- ✅ 包含技术细节要求（数据结构、路由、模板）
- ✅ 强调代码风格一致性（"和当前风格一致"）
- ✅ 有明确的验收标准（启动命令、URL、验证步骤）
- ✅ 符合真实开发场景（博客系统确实需要浏览历史）
- ✅ 代码量适中（几十到两三百行）

**评分结果：**
- Kimi_k2: 交付完整性 ⭐⭐⭐⭐⭐，工具调用 ⭐⭐⭐⭐⭐，用户体验 ⭐⭐⭐⭐
- doubao-1.6: 交付完整性 ⭐⭐⭐⭐，工具调用 ⭐⭐⭐⭐，用户体验 ⭐⭐⭐⭐
- **GSB 结果：** Kimi_k2 更好（超预期完成，工具调用更高效）

---

## ❌ 常见被拒案例

### 1. 不符合真实使用场景

**反例：**
```
❌ 在弹窗组件项目中去实现一个游戏
```

**为什么被拒：**
在游戏中增加弹窗组件是合理的，但在弹窗项目中实现游戏不符合真实场景。

---

### 2. 多题目重复（套题）

**反例：**
```
❌ 在当前 repo 中去实现贪吃蛇、超级玛丽、连连看、坦克大战、时钟、日历、录音机…
```

**为什么被拒：**
这是典型的"游戏合集套题"，可以无限重复，不符合评估标准。遇到这种会直接拒绝！

---

### 3. 复杂系统堆砌

**反例：**
```
❌ 实现基于预测性缓存预热与自适应资源调度的智能缓存系统
❌ 实现支持事务性操作的强一致性缓存机制
❌ 开发基于硬件特性感知的异构缓存优化引擎
```

**为什么被拒：**
不符合日常使用场景，过于学术化，建议拆分成独立的、更实际的需求。

---

### 4. 过于简单

**反例：**
```
❌ 请将网站设置为暗色模式
❌ 翻译当前项目
```

**为什么被拒：**
任务过于简单，无法触发 Model Breaking，不满足难度要求。

---

### 5. 不规范的描述语言

**反例：**
```
❌ 英文 prompt（除非项目本身是英文项目）
❌ Markdown/JSON 格式的 prompt（AI 味过浓）
```

**修改建议：**
使用自然、口语化的中文描述，像和同事沟通一样。

---

## 🔧 专业名词解释

| 名词 | 解释 |
|------|------|
| **白名单账号** | 在 Trae 备案过的测试账号，由管理员提供 |
| **Session ID** | 与 Trae 单次会话的唯一 ID<br>获取方式：双击 AI 对话头像"Builder" |
| **Model Breaking** | 模型在完成任务时出现必须人工介入的情况<br>• 需要人工介入 = 触发了 Model Breaking<br>• 无需人工介入 = 未触发 Model Breaking |
| **Pull Request (PR)** | GitHub 合并请求，需要提供链接地址<br>示例：`https://github.com/royIdoodle/xyz/pull/1` |
| **COT (Chain-of-Thought)** | 思维链/推理链，模型的推理过程<br>COT 长度 = 每个推理块的完整内容长度 |
| **对话轮次/交付轮次** | 每次在对话框中与 Trae 的交流<br>注意：点击"继续"或"重试"按钮不计入轮次 |
| **任务完成轮次** | 从开始到任务完成所使用的对话轮次 |

---

## 📝 任务提交检查清单

### 关键字段要求

| 字段 | 说明 | ❌ 审核不通过的情况 |
|------|------|---------------------|
| **Prompt** | 首轮的 Prompt<br>倡导贴近日常工作内容，存在适度的复杂性；<br>不建议故意刁难 Trae | • 从 0 开始构建一个项目<br>• 无限套用的项目（游戏合集）<br>• 复杂功能只提供一句话，不提供完善的 prompt |
| **Github Repo 地址** | 示例：`https://github.com/satnaing/shadcn-admin`<br>禁止再使用 `shadcn-admin` 这个仓库<br>如通过试标，后续项目会限制同一 repo 的使用次数 | • 用大模型生成的项目<br>• repo 使用次数超限 |
| **repo 介绍** | 用几句话高度概括当前项目的主要功能、技术栈等关键信息 | • 空 |
| **Github PR 链接** | 示例：`https://github.com/royIdoodle/shadcn-admin/pull/1` | • 空<br>• 格式不正确（缺少 /pull/数字）<br>• Gitee 的 PR 链接 |
| **Session ID** | 仅填写首轮的 Session ID<br>获取方式：双击 AI 对话头像"Builder" | • 空<br>• 格式不正确<br>• 多个 Session ID |

---

## 🚀 操作流程总结

### Step 1: 项目准备
- Fork GitHub 开源项目到自己的账号
- 确保仓库权限设置为 **public**
- 也可以使用自己的项目（但不能是空项目）

### Step 2: 构造任务
- 按照本文档的"任务构造标准"设计任务
- 确保任务分类属于：界面功能 / 运行时异常修复 / 业务逻辑
- 控制对话轮次：试标最多 2 轮，正式项目最多 3 轮

### Step 3: 提交代码并创建 PR
- **重要：** PR 的标题必须是 Session ID，否则不予通过
- 不要直接提交到 main 分支
- 不要漏掉 Trae 新增的文件
- 合并请求要指向自己的仓库，不要指向开源项目

### Step 4: 评估交付结果
- 真实运行 Trae 提交的代码
- 按照三大维度打分：交付完整性、工具调用、用户体验
- 撰写详细的 GSB 对比分析

---

## 💡 AI 助手输出模板

当用户要求生成任务建议时，使用以下模板输出：

```markdown
## 🎯 推荐方案（按难度排序）

### 方案 1：[功能名称] ⭐⭐⭐⭐（推荐）

[自然语言描述的 prompt，3-5 段，包含：
1. 背景说明
2. 功能需求（3-5 个具体点）
3. 技术要求
4. 验收标准]

**适合理由：**
- 符合真实业务需求（具体说明）
- 涉及模块：[列举]
- 代码量预估：[范围]
- 人工开发耗时：[范围]
- 属于"[任务分类]"类型

---

### 方案 2：[功能名称] ⭐⭐⭐⭐⭐

[类似格式]

---

### 方案 3：[功能名称] ⭐⭐⭐⭐

[类似格式]

---

## 💡 使用建议

1. **如果是试标（2 轮对话限制）**：推荐**方案 X**，[理由]
2. **如果想测试模型极限（正式项目）**：推荐**方案 X**，[理由]
3. **如果想测试 [特定能力]**：推荐**方案 X**，[理由]

---

## ⚠️ 注意事项

- 这些 prompt 都符合**真实开发场景**，不是刻意刁难
- 任务复杂度适中，**避免过于简单或从 0 开始构建**
- 都包含了**明确的验证步骤**，方便评估交付完整性
- 描述使用**自然语言**，符合日常工作沟通习惯
```

---

## 🔍 常见问题

### Q: 配置 PPE 测试环境后依然看不到测试模型？
**A:**
1. 【推荐】关闭 Trae CN，等待 10 分钟后再进入（80% 用户通过此方法解决）
2. 检查配置文件内容是否正确
3. 确认登录的账号是否为开通白名单的账号（UID: 1875084792839892）
4. 检查是否启用的是 Trae CN
5. 确认是否操作过 reload
6. 重新安装 Trae CN

### Q: 如何双开/多开 Trae CN？
**A:**
1. 在电脑中创建多个不同的文件夹（如 A、B）
2. 分别在新建的文件夹中 `git clone` 相同的项目
3. 通过 Trae 分别打开文件夹中的两个不同项目，并切换到不同的模型
4. 将同一个 prompt 输入给两个 Trae，然后分别观察这两个模型的表现

**目的：** 双开时，不会互相影响。

### Q: 遇到"AI 问答今日使用超限"如何解决？
**A:** 联系群里管理人员更换一个手机号登录 Trae CN。

---

## 📌 版本信息

- **文档版本：** v1.0
- **适用范围：** Trae CN 模型评估项目（试标 + 正式项目）
- **最后更新：** 2025-12-26
- **维护者：** AI Assistant

---

## 📚 相关资源

- Trae 官方文档：https://docs.trae.cn/
- 问题排查指南：https://docs.trae.cn/ide/troubleshooting
- 工具调用日志分析工具：https://trae.myuan.fun/
- Trae CN 下载地址：https://www.trae.cn/ide/download
